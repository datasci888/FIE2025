{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466845d0-ac37-42d2-8566-76d951f0e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-storage google-cloud-vision google-generativeai PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e80ce1-f909-4fb4-8151-5496c3d4fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import storage\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Initialize storage client\n",
    "storage_client = storage.Client()\n",
    "bucket_name = \"books-mcq\"\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "print(\"Storage client initialized for bucket:\", bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997b95c-e400-4af1-88dc-ace701a2cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all parsed chunk files\n",
    "chunk_files = [blob.name for blob in bucket.list_blobs(prefix=\"parsed_files/\") \n",
    "               if blob.name.endswith(\"_chunks.json\")]\n",
    "\n",
    "if chunk_files:\n",
    "    print(f\"Found {len(chunk_files)} parsed chunk files:\")\n",
    "    for i, file in enumerate(chunk_files, 1):\n",
    "        print(f\"{i}. {file}\")\n",
    "else:\n",
    "    print(\"No parsed chunk files found in bucket under 'parsed_files/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed87be-fb65-4d08-ac34-0ab957d4d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List PDFs in bucket\n",
    "pdf_files = [blob.name for blob in bucket.list_blobs() if blob.name.lower().endswith(\".pdf\")]\n",
    "print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "for i, file in enumerate(pdf_files, 1):\n",
    "    print(f\"{i}. {file}\")\n",
    "\n",
    "# List existing chunk files\n",
    "chunk_files = [blob.name for blob in bucket.list_blobs(prefix=\"parsed_files/\") \n",
    "               if blob.name.endswith(\"_chunks.json\")]\n",
    "print(f\"\\nFound {len(chunk_files)} parsed chunk files:\")\n",
    "for i, file in enumerate(chunk_files, 1):\n",
    "    print(f\"{i}. {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c8d92-d696-4036-9eb3-f26af54b9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from crewai_tools import FileReadTool, DirectoryReadTool\n",
    "from crewai.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Type\n",
    "from google import genai\n",
    "from google.genai.types import EmbedContentConfig\n",
    "from google.cloud import storage\n",
    "import json\n",
    "\n",
    "# GCS & Gemini setup\n",
    "vector_prefix = \"vector_store_cleaned_new/\"\n",
    "client = genai.Client(vertexai=True, project=\"summarize-39810\", location=\"us-central1\")\n",
    "bucket = storage.Client().bucket(\"books-for-mcq\")\n",
    "\n",
    "# Input schema for duplicate checker\n",
    "class DuplicateCheckerInput(BaseModel):\n",
    "    chunks: List[Dict] = Field(..., description=\"List of text chunks to check for duplicates\")\n",
    "\n",
    "# Custom tool for duplicate detection\n",
    "class DuplicateCheckerTool(BaseTool):\n",
    "    name: str = \"DuplicateChecker\"\n",
    "    description: str = \"Detects duplicate chunk IDs or text.\"\n",
    "    args_schema: Type[BaseModel] = DuplicateCheckerInput\n",
    "\n",
    "    def _run(self, chunks: List[Dict]) -> Dict:\n",
    "        seen_ids, seen_texts = set(), set()\n",
    "        duplicates = {\"ids\": [], \"texts\": []}\n",
    "        unique_chunks = []\n",
    "        for chunk in chunks:\n",
    "            cid, text = chunk[\"chunk_id\"], chunk[\"text\"]\n",
    "            if cid in seen_ids:\n",
    "                duplicates[\"ids\"].append(cid)\n",
    "            elif text in seen_texts:\n",
    "                duplicates[\"texts\"].append(cid)\n",
    "            else:\n",
    "                seen_ids.add(cid)\n",
    "                seen_texts.add(text)\n",
    "                unique_chunks.append(chunk)\n",
    "        return {\"unique_chunks\": unique_chunks, \"duplicates\": duplicates}\n",
    "\n",
    "# Embedding function\n",
    "def generate_embeddings(texts):\n",
    "    try:\n",
    "        response = client.models.embed_content(\n",
    "            model=\"text-embedding-005\",\n",
    "            contents=texts,\n",
    "            config=EmbedContentConfig(task_type=\"RETRIEVAL_DOCUMENT\", output_dimensionality=768)\n",
    "        )\n",
    "        return [response.embeddings[i].values for i in range(len(texts))]\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding: {e}\")\n",
    "        return [None] * len(texts)\n",
    "\n",
    "# Load pre-parsed chunks from GCS\n",
    "def load_parsed_chunks():\n",
    "    prefix = \"parsed_files/\"\n",
    "    chunk_files = [b.name for b in bucket.list_blobs(prefix=prefix) if b.name.endswith(\"_chunks.json\")]\n",
    "    all_chunks = []\n",
    "    for fname in chunk_files:\n",
    "        blob = bucket.blob(fname)\n",
    "        all_chunks.extend(json.loads(blob.download_as_text()))\n",
    "    return all_chunks\n",
    "\n",
    "# Define agent\n",
    "data_engineer = Agent(\n",
    "    role=\"Data Engineer\",\n",
    "    goal=\"Deduplicate and embed pre-parsed chunks\",\n",
    "    backstory=\"Expert in data pipelines, GCS, and embeddings.\",\n",
    "    tools=[FileReadTool(), DirectoryReadTool(), DuplicateCheckerTool()],\n",
    "    llm=llm,  # ‚úÖ Pass your LLM here\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# Define tasks\n",
    "task_deduplicate = Task(\n",
    "    description=\"Load all chunked JSONs from 'parsed_files/', deduplicate chunk_ids and text.\",\n",
    "    expected_output=\"Deduplicated chunk list and report of duplicates.\",\n",
    "    agent=data_engineer\n",
    ")\n",
    "\n",
    "task_vectorize = Task(\n",
    "    description=\"Generate embeddings for all unique chunks using text-embedding-005 and save with metadata.\",\n",
    "    expected_output=\"Vector list with chunk_id, embedding, and metadata.\",\n",
    "    agent=data_engineer\n",
    ")\n",
    "\n",
    "task_verify = Task(\n",
    "    description=\"Verify total unique chunks (~20,944) and locate chunks mentioning 'reinforcement learning'.\",\n",
    "    expected_output=\"Count and sample of RL-relevant chunks.\",\n",
    "    agent=data_engineer\n",
    ")\n",
    "\n",
    "# Assemble crew\n",
    "crew = Crew(\n",
    "    agents=[data_engineer],\n",
    "    tasks=[task_deduplicate, task_vectorize, task_verify],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Execute\n",
    "print(\"Starting CrewAI pipeline...\")\n",
    "result = crew.kickoff()\n",
    "print(\"Pipeline complete. Final result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c88094-b622-41da-b909-8df02d323ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding function / Testing with Weaviate\n",
    "def generate_embeddings(texts):\n",
    "    try:\n",
    "        response = client.models.embed_content(\n",
    "            model=\"text-embedding-005\",\n",
    "            contents=texts,\n",
    "            config=EmbedContentConfig(task_type=\"RETRIEVAL_DOCUMENT\", output_dimensionality=768)\n",
    "        )\n",
    "        return [response.embeddings[i].values for i in range(len(texts))]\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding: {e}\")\n",
    "        return [None] * len(texts)\n",
    "\n",
    "# Test chunk\n",
    "test_chunk = {\n",
    "    \"chunk_id\": \"test_chunk_1\",\n",
    "    \"text\": \"Reinforcement learning is a machine learning paradigm where agents learn by interacting with an environment.\",\n",
    "    \"metadata\": {\"source\": \"test\", \"page\": 1}\n",
    "}\n",
    "\n",
    "# Create or get collection with nested metadata properties\n",
    "collection_name = \"TestBookChunks\"\n",
    "if weaviate_client.collections.exists(collection_name):\n",
    "    weaviate_client.collections.delete(collection_name)  # Reset for clean test\n",
    "weaviate_client.collections.create(\n",
    "    name=collection_name,\n",
    "    vectorizer_config=None,  # Custom embeddings\n",
    "    properties=[\n",
    "        Property(name=\"chunk_id\", data_type=DataType.TEXT),\n",
    "        Property(name=\"text\", data_type=DataType.TEXT),\n",
    "        Property(\n",
    "            name=\"metadata\",\n",
    "            data_type=DataType.OBJECT,\n",
    "            nested_properties=[\n",
    "                Property(name=\"source\", data_type=DataType.TEXT),\n",
    "                Property(name=\"page\", data_type=DataType.INT)\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Store the test chunk\n",
    "collection = weaviate_client.collections.get(collection_name)\n",
    "embedding = generate_embeddings([test_chunk[\"text\"]])[0]\n",
    "if embedding:\n",
    "    with collection.batch.dynamic() as batch:\n",
    "        batch.add_object(\n",
    "            properties={\n",
    "                \"chunk_id\": test_chunk[\"chunk_id\"],\n",
    "                \"text\": test_chunk[\"text\"],\n",
    "                \"metadata\": test_chunk[\"metadata\"]  # Nested dict\n",
    "            },\n",
    "            vector=embedding)\n",
    "    print(f\"Stored test chunk in Weaviate '{collection_name}'\")\n",
    "else:\n",
    "    print(\"Failed to generate embedding for test chunk\")\n",
    "\n",
    "# Verify storage\n",
    "total_count = collection.aggregate.over_all(total_count=True).total_count\n",
    "print(f\"Total objects in '{collection_name}': {total_count}\")\n",
    "\n",
    "# Query test\n",
    "query = \"reinforcement learning\"\n",
    "query_embedding = generate_embeddings([query])[0]\n",
    "response = collection.query.near_vector(near_vector=query_embedding,limit=1,return_metadata=[\"distance\"])\n",
    "if response.objects:\n",
    "    obj = response.objects[0]\n",
    "    print(\"\\nQueried result:\")\n",
    "    print(f\"Chunk ID: {obj.properties['chunk_id']}\")\n",
    "    print(f\"Text: {obj.properties['text']}\")\n",
    "    print(f\"Metadata: {obj.properties['metadata']}\")\n",
    "    print(f\"Distance: {obj.metadata.distance:.4f}\")\n",
    "else:\n",
    "    print(\"No results found in query\")\n",
    "\n",
    "# Cleanup\n",
    "weaviate_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407e6a4-3bcc-4549-aa88-f2cc7fcb8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 chunks test working code with weaviate for deveopment purpose\n",
    "from crewai import Agent, Task, Crew\n",
    "from google.cloud import storage\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.config import Property, DataType\n",
    "import json\n",
    "\n",
    "# GCS & Weaviate setup\n",
    "bucket = storage.Client().bucket(\"books-for-mcq\")\n",
    "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=\"https://oejgmjaer1on7rjlm7lw.c0.us-east1.gcp.weaviate.cloud\",\n",
    "    auth_credentials=Auth.api_key(\"jSNt4ttbezx7KXcnwq50Ll2qFxrHsYUE59V\")\n",
    ")\n",
    "\n",
    "# Load 5 chunks\n",
    "blob = bucket.blob(\"parsed_files/cleaned_combined_chunks.json\")\n",
    "chunks = json.loads(blob.download_as_text()).get(\"chunks\", [])[:5]\n",
    "# Define a custom tool for Weaviate storage\n",
    "class WeaviateStoreTool(BaseTool):\n",
    "    name: str = \"WeaviateStore\"\n",
    "    description: str = \"Stores a chunk in Weaviate with chunk_id, text, and metadata.\"\n",
    "\n",
    "    def _run(self, chunk: dict) -> str:\n",
    "        required_keys = {\"chunk_id\", \"text\", \"metadata\"}\n",
    "        if not all(k in chunk for k in required_keys):\n",
    "            raise ValueError(f\"Chunk missing required keys: {required_keys - chunk.keys()}\")\n",
    "        collection = weaviate_client.collections.get(\"BookChunks\")\n",
    "        with collection.batch.dynamic() as batch:\n",
    "            batch.add_object(properties=chunk, vector=[0.1]*768)\n",
    "        return f\"Stored chunk {chunk['chunk_id']}\"\n",
    "\n",
    "# Agent and Task\n",
    "agent = Agent(\n",
    "    role=\"Data Engineer\",\n",
    "    goal=\"Store chunks\",\n",
    "    backstory=\"Expert in managing data pipelines and vector storage.\",\n",
    "    llm=\"openrouter/deepseek/deepseek-chat\"\n",
    ")\n",
    "task = Task(\n",
    "    description=\"Store these 5 chunks in Weaviate one by one: \" + json.dumps(chunks),\n",
    "    expected_output=\"Stored 5 chunks\",\n",
    "    agent=agent,\n",
    "    tools=[WeaviateStoreTool()]\n",
    ")\n",
    "# Crew execution\n",
    "crew = Crew(agents=[agent], tasks=[task], verbose=True)\n",
    "result = crew.kickoff()\n",
    "print(\"Result:\", result)\n",
    "\n",
    "# Verify\n",
    "collection = weaviate_client.collections.get(\"BookChunks\")\n",
    "print(\"Total in Weaviate:\", collection.aggregate.over_all(total_count=True).total_count)\n",
    "weaviate_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad6ff8-a020-4039-964a-5120afad5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Query FAISS Index from Vectors Stored in GCS\"\"\"\n",
    "\n",
    "from google.cloud import storage\n",
    "from google import genai\n",
    "from google.genai.types import EmbedContentConfig\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Setup\n",
    "project_id = \"summarize-39910\"\n",
    "location = \"us-central1\"\n",
    "bucket_name = \"books-mcq\"\n",
    "vector_prefix = \"vector_store_cleaned/\"\n",
    "\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=project_id,\n",
    "    location=location\n",
    ")\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "# Load first vector file\n",
    "for blob in bucket.list_blobs(prefix=vector_prefix, max_results=1):\n",
    "    print(f\"\\nüìÇ Loading vector file: {blob.name}\")\n",
    "    vector_data = json.loads(blob.download_as_string())\n",
    "    break\n",
    "\n",
    "# Build FAISS index\n",
    "dimension = 768\n",
    "index = faiss.IndexHNSWFlat(dimension, 32)\n",
    "index.hnsw.efConstruction = 100\n",
    "\n",
    "ids = []\n",
    "metas = []\n",
    "\n",
    "print(f\"üìä Building index from {len(vector_data)} vectors...\")\n",
    "for item in vector_data:\n",
    "    vector = np.array(item['embedding'], dtype='float32')\n",
    "    index.add(np.expand_dims(vector, axis=0))\n",
    "    ids.append(item['chunk_id'])\n",
    "    metas.append(item.get('metadata', {}))\n",
    "\n",
    "print(f\"‚úÖ FAISS index built with {len(ids)} vectors.\")\n",
    "\n",
    "# Query\n",
    "def generate_embedding(text):\n",
    "    response = client.models.embed_content(\n",
    "        model=\"text-embedding-005\",\n",
    "        contents=[text],\n",
    "        config=EmbedContentConfig(task_type=\"RETRIEVAL_DOCUMENT\", output_dimensionality=768)\n",
    "    )\n",
    "    return response.embeddings[0].values\n",
    "\n",
    "query_text = input(\"\\nüîç Enter your query (e.g., 'What is reinforcement learning?'):\\n> \")\n",
    "query_embedding = np.array([generate_embedding(query_text)], dtype='float32')\n",
    "index.hnsw.efSearch = 100\n",
    "distances, indices = index.search(query_embedding, k=5)\n",
    "\n",
    "print(f\"\\nüîé Top 5 Matches for: '{query_text}'\")\n",
    "for rank, (idx, dist) in enumerate(zip(indices[0], distances[0]), 1):\n",
    "    print(f\"\\n#{rank}: Chunk ID: {ids[idx]}\")\n",
    "    print(f\"   Distance: {dist:.4f}\")\n",
    "    print(f\"   Metadata: {metas[idx]}\")\n",
    "\n",
    "# Load original chunks to get text content\n",
    "chunk_blob_name = blob.name.replace(\"_vectors.json\", \"_chunks.json\").replace(\"vector_store_cleaned_new/\", \"parsed_files/\")\n",
    "chunk_blob = bucket.blob(chunk_blob_name)\n",
    "chunk_data = json.loads(chunk_blob.download_as_string())\n",
    "chunk_dict = {c['chunk_id']: c['text'] for c in chunk_data['chunks']}\n",
    "\n",
    "# Show text for matched chunks\n",
    "print(f\"\\nüìÑ Text content of top 5 matched chunks:\\n\")\n",
    "for rank, idx in enumerate(indices[0]):\n",
    "    cid = ids[idx]\n",
    "    text = chunk_dict.get(cid, \"[Text not found]\")\n",
    "    print(f\"#{rank+1}: Chunk ID: {cid}\")\n",
    "    print(f\"üìò Text Preview: {text[:30000]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb83e1-7737-4b5d-ac51-b172f3aaf552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment whole dataset\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Initialize storage client\n",
    "bucket_name = \"books-mcq\"\n",
    "file_path = \"Quizxlsx/MCQsDataset.xlsx\"\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(file_path)\n",
    "\n",
    "# Download Excel file into memory\n",
    "excel_bytes = blob.download_as_bytes()\n",
    "df = pd.read_excel(io.BytesIO(excel_bytes))\n",
    "mcq_json = df.to_dict(orient=\"records\")\n",
    "master_mcq_dataset_to_refer_string = json.dumps(mcq_json, indent=2)\n",
    "print(master_mcq_dataset_to_refer_string[:1000])  \n",
    "\n",
    "# Show basic info\n",
    "print(f\"‚úÖ Loaded sheet with shape: {df.shape}\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd905e8a-3db9-440e-aa4d-73812159abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MCQ for FAANG interview for Artificial Intelligence\n",
    "# Artificial Intelligence scientist applied role intervview Qs MCQs generate 10 Qsfrom google.genai.types import HttpOptions\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import time\n",
    "start = time.time()\n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\n",
    "storage_client = storage.Client()\n",
    "bucket_name = \"books-for-mcq\"\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "# Query FAISS and generate MCQ with user input\n",
    "def generate_mcq(index, ids, metadata_list, top_k=3):\n",
    "    query_text = input(\"Enter your query (e.g., 'Generate MCQ for FAANG interview for data scientist position'): \")\n",
    "    \n",
    "    query_embedding = np.array([generate_embedding(query_text)], dtype='float32')\n",
    "    index.hnsw.efSearch = 100\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    print(f\"\\nTop {top_k} matches for '{query_text}':\")\n",
    "    relevant_chunks = []\n",
    "    # Reload chunk data from GCS\n",
    "    for blob in bucket.list_blobs(prefix='parsed_files/', max_results=1):\n",
    "        if blob.name.endswith('_chunks.json'):\n",
    "            chunk_data = json.loads(blob.download_as_string())\n",
    "            break\n",
    "    \n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        chunk_info = f\"Chunk ID: {ids[idx]}, Distance: {dist}, Text: {chunk_data['chunks'][idx].get('text', 'No text')}\"\n",
    "        print(f\"#{i+1}: {chunk_info}\")\n",
    "        relevant_chunks.append(chunk_data['chunks'][idx]['text'])\n",
    "\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Based on the following text chunks and the user query '{query_text}', and refering to the master dataset:\"{master_mcq_dataset_to_refer_string}\", generate a multiple-choice question (MCQ) with 4 options (A, B, C, D). Provide the question, options, and the correct answer in this format:\\n\"\n",
    "        \"Question: [Your question here]\\n\"\n",
    "        \"A) [Option A]\\n\"\n",
    "        \"B) [Option B]\\n\"\n",
    "        \"C) [Option C]\\n\"\n",
    "        \"D) [Option D]\\n\"\n",
    "        \"Correct Answer: [Letter]\\n\"\n",
    "        \"Explanation: [Brief justification based on chunk content]\\n\"\n",
    "        \"Source Chunk IDs: [List of chunk_ids used for answer]\\n\\n\"\n",
    "        \"Example:\\n\"\n",
    "        \"Question: What is the primary goal of reinforcement learning?\\n\"\n",
    "        \"A) Minimize prediction error\\n\"\n",
    "        \"B) Maximize cumulative reward\\n\"\n",
    "        \"C) Classify data\\n\"\n",
    "        \"D) Reduce variance\\n\"\n",
    "        \"Correct Answer: B\\n\"\n",
    "        \"Explanation: This is stated in the text where it discusses reinforcement learning agents maximizing expected reward.\\n\"\n",
    "        \"Source Chunk IDs: 122_0, 1041_5\\n\\n\"\n",
    "        \"Source Chunk Text excerpt used: \\\"Excerpt in 100 words\\\".\\n\"\n",
    "        \"Master Dataset to refer before producing the MCQs: {master_mcq_dataset_to_refer_string} and then refer chunks for context;\n",
    "        \"Chunks:\\n\" + \"\\n\\n\".join({relevant_chunks})\n",
    "        Return first all MCQs generated by you, then chunks refered as above as a whole separate section.\n",
    "        Return in pure json format with keys as MCQ, Reference_dataset, Reference_chunks and corresponding values. \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-1.5-pro\", #  gemini-2.0-flash\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGenerated MCQ:\")\n",
    "    print(response.text)\n",
    "    end = time.time()\n",
    "    print(f\"‚è±Ô∏è Response Time: {end - start:.2f} seconds\")\n",
    "\n",
    "# Run it \n",
    "if __name__ == \"__main__\":\n",
    "    generate_mcq(index, ids, metadata_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e566f-28aa-4548-babc-eb5b4328ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load Cache \n",
    "from google.cloud import storage\n",
    "from google import genai\n",
    "from google.genai.types import HttpOptions, Content, CreateCachedContentConfig\n",
    "import pandas as pd\n",
    "import io\n",
    "import json\n",
    "\n",
    "# Initialize clients\n",
    "storage_client = storage.Client()\n",
    "genai_client = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\n",
    "bucket = storage_client.bucket(\"books-for-mcq\")\n",
    "\n",
    "# Load master dataset from GCS\n",
    "blob = bucket.blob(\"Quizxlsx/MCQsDatasetTIll3800.xlsx\")\n",
    "excel_bytes = blob.download_as_bytes()\n",
    "df = pd.read_excel(io.BytesIO(excel_bytes))\n",
    "master_mcq_dataset_to_refer_string = json.dumps(df.to_dict(orient=\"records\"), indent=2)\n",
    "print(f\"Loaded dataset with {len(df)} records, preview: {master_mcq_dataset_to_refer_string[:100]}...\")\n",
    "\n",
    "# Create cache\n",
    "# Create cache\n",
    "system_instruction = \"You are an expert MCQ generator for FAANG interviews, using a master dataset and book chunks.\"\n",
    "\n",
    "contents = [\n",
    "    Content(\n",
    "        role=\"user\",\n",
    "        parts=[{\"text\": master_mcq_dataset_to_refer_string}]\n",
    "    )\n",
    "]\n",
    "\n",
    "content_cache = genai_client.caches.create(\n",
    "    model=\"gemini-1.5-flash-002\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        contents=contents,\n",
    "        system_instruction=system_instruction,\n",
    "        display_name=\"master_mcq_dataset_cache\",\n",
    "        ttl=\"86400s\"  # 24 hours\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Cache created: {content_cache.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb18a36-9df5-4567-8109-3f4f3b220264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Cache\n",
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "\n",
    "# Initialize client with beta version\n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1beta1\"))\n",
    "\n",
    "# List all caches in the current project and location\n",
    "content_cache_list = client.caches.list()\n",
    "\n",
    "# Loop through and print details\n",
    "for content_cache in content_cache_list:\n",
    "    print(f\"üß† Cache: {content_cache.name}\")\n",
    "    print(f\"üì¶ Model: {content_cache.model}\")\n",
    "    print(f\"üïí Last updated: {content_cache.update_time}\")\n",
    "    print(f\"‚åõ Expires: {content_cache.expire_time}\")\n",
    "    \n",
    "    if hasattr(content_cache, \"usage_metadata\"):\n",
    "        usage = content_cache.usage_metadata\n",
    "        print(f\"üßÆ Token Count: {usage.total_token_count}\")\n",
    "        print(f\"üñºÔ∏è Image Count: {usage.image_count}\")\n",
    "        print(f\"üéûÔ∏è Video Duration: {usage.video_duration_seconds}\")\n",
    "        print(f\"üîä Audio Duration: {usage.audio_duration_seconds}\")\n",
    "    print(\"‚îÄ\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66658b13-ca51-4e6a-9163-d5014ef91857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache reduced 85s to 21s response time\n",
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig, HttpOptions\n",
    "import time \n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1beta1\"))\n",
    "\n",
    "# Use the cache created earlier\n",
    "cache_name = \"projects/493443117630/locations/us-central1/cachedContents/821217382044999680\"\n",
    "query_text = \"Generate a MCQ for FAANG data scientist interview on supervised learning.\"\n",
    "\n",
    "start = time.time()\n",
    "# Call Gemini using cached master dataset\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-flash-002\",\n",
    "    contents=[{\"role\": \"user\", \"parts\": [{\"text\": query_text}]}],\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cache_name,\n",
    "    ),\n",
    ")\n",
    "time_taken = time.time() - start\n",
    "\n",
    "# Display response\n",
    "print(\"üéØ Generated MCQ:\\n\")\n",
    "print(response.text)\n",
    "\n",
    "print(f\"Time taken: {time_taken} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74549fb-841b-47fd-81ac-bd2cc6786291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache + vectors reduced 85s to 19s response time\n",
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig, HttpOptions\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Initialize Gemini client (v1beta1 for cache support)\n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1beta1\"))\n",
    "\n",
    "# Pre-set: Cache with master MCQ dataset\n",
    "cache_name = \"projects/493443117630/locations/us-central1/cachedContents/821217538204499680\"\n",
    "\n",
    "# Get user query\n",
    "query_text = \"Generate 3 MCQs for FAANG interview with references.\"\n",
    "\n",
    "# Use FAISS to retrieve top-k relevant chunks\n",
    "query_embedding = np.array([generate_embedding(query_text)], dtype='float32')\n",
    "index.hnsw.efSearch = 100\n",
    "distances, indices = index.search(query_embedding, 5)\n",
    "\n",
    "# Load chunks from GCS\n",
    "for blob in bucket.list_blobs(prefix='parsed_files/', max_results=1):\n",
    "    if blob.name.endswith('_chunks.json'):\n",
    "        chunk_data = json.loads(blob.download_as_string())\n",
    "        break\n",
    "\n",
    "# Gather top chunks and their IDs\n",
    "retrieved_chunks = []\n",
    "retrieved_chunk_ids = []\n",
    "for dist, idx in zip(distances[0], indices[0]):\n",
    "    text = chunk_data['chunks'][idx].get(\"text\", \"\")\n",
    "    cid = chunk_data['chunks'][idx].get(\"chunk_id\", \"\")\n",
    "    retrieved_chunks.append(text)\n",
    "    retrieved_chunk_ids.append(cid)\n",
    "\n",
    "# Prepare the prompt for Gemini\n",
    "prompt_text = f\"\"\"\n",
    "User Query: {query_text}\n",
    "\n",
    "Below are relevant textbook chunks retrieved via vector search:\n",
    "{json.dumps(retrieved_chunks[:3], indent=2)}\n",
    "\n",
    "Provide 3 MCQs in JSON format based on the above chunks and your cached master dataset.\n",
    "Source preferred (as in cache : {cache_name}: use interviewquery, NeetcodeYT, Top 5 Algorithms Qs YT, Algorithm Class Notes,interviewquery.com\n",
    "Topic preferred: Data Structures, Machine learning and Algorithms. \n",
    "After selection, apply a real life use case as it would be used in a research in Thomas Reuters.\n",
    "Return format:\n",
    "{{\n",
    "  \"MCQ\": [...],\n",
    "  \"Source\": Source,\n",
    "  \"Topic\": topics used,\n",
    "  \"Reference_dataset\": \"Gemini cache used: {cache_name}\",\n",
    "  \"Reference_chunks\": {json.dumps(retrieved_chunk_ids)}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Call Gemini model\n",
    "start_time = time.time()\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-flash-002\",\n",
    "    contents=[{\"role\": \"user\", \"parts\": [{\"text\": prompt_text}]}],\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cache_name,\n",
    "    ),\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# Output\n",
    "print(\"üéØ Generated Response:\\n\")\n",
    "print(response.text)\n",
    "print(f\"‚è±Ô∏è Time taken: {end_time - start_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
